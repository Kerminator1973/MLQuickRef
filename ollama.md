# Ollama - локальные LLM

[Ollama](https://ollama.com/) -  это инструмент для простого локального запуска больших языковых моделей (LLM) одной командой. Его часто сравнивают с Docker — но применительно к AI‑моделям: достаточно выполнить одну команду, и модель запускается без необходимости разбираться в зависимостях, конфигурациях и драйверах.

Пример команды:

```shell
ollama run llama2
```

Ollama работает с рядом известных открытых LLM, в том числе:

- LLaMA (включая Llama 3)
- Mistral
- CodeLlama

Ключевое свойство - автоматическая оптимизация:

- определяет доступные ресурсы (CPU/GPU)
- подстраивает загрузку модели под возможности оборудования
- поддерживает квантование и оптимизацию моделей

Небольшие модели можно запускать и на бюджетных процессорах, например, на Core i3 13100, но рекомендуется использовать только 3B‑модели (например, llama3.2:3b, phi3:mini):

```shell
# Запуск Phi‑3 Mini
ollama run phi3:mini

# Запуск Llama 3.2 3B
ollama run llama3.2:3b

# Запуск квантованной Mistral 7B
ollama run mistral:7b-instruct-v0.1-q4_k_m
```

Core i3‑13100 подходит для экспериментов с Ollama и LLM, но не более того.

Настройки для повышения производительности:

- количество параллельных запросов не более 1–2
- квантованные версии моделей (формат GGUF, 4–8 бит)
- все фоновые ресурсоёмкие приложения должны быть закрыты
- для снижения нагрузки следует уменьшить размер контекста. Например: `--num-ctx 2048`

## Hugging Face

[Hugging Face](https://huggingface.co/) - это тоже самое для машинного обучения, что GitHub для кода. На Hugging Face размещаются ИИ модели, решающие различные задачи. В частности, есть много узкоспециализированных ИИ моделей, например, только для генерации кода на ИИ. Узкоспециализированные модели могут эффективно решать сложные задачи, расходуя кратно меньшее количество ресурсов, чем при использовании универсальных моделей.

>Hugging Face после нескольких лет плодотворного сотрудничества купила команду llama.cpp. Предполагается, что ПО останется открытым, но у разработчиков появится больше ресурсов для совершенствования продуктов.

В репозитариях Hugging Face находятся как модели, так и Datasets.

>Hugging Face - это эмодзи "обнимашка". Миссия Hugging Face: "ИИ для исследователей, демократизация ИИ".

Для запуска моделей с Hugging Face необходима Ollama. Можно зарузить Docker-контейнер, но для этого нужне API-ключ (DASHSCOPE_API_KEY), а также поддержку containerd engine в Docker.

Благодаря использованию карточек моделей и поиска, на Hugging Face довольно удобно искать специализированные модели.

## Установка Ollama

Команда установки Ollama в Linux:

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

Проверка версии:

```shell
ollama --version
```

Инсталлятор клиента Ollama в Microsoft Windows загружается напрямую с сайта. Его развер составляет 1.2 ГБ. Без заведения аккаунта доступно всего несколько моделей. Размер модели gemma3:4b - 3.1 ГБ.

Ollama выполняет модель gemma3:4b вполне шустро, приблизительно, со скоростью чуть медленнее вывода на Duck.io в web, на компьютере с процессором Core i3 13100, без дискретной графики. При этом процессор нагревается до 60+ градусов.

Приложения "ollama app.exe" и "ollama.exe" находятся в папке: `C:\Users\[User]\AppData\Local\Programs\Ollama\ollama app.exe`

На портале Hugging Face можно найти модель в формате GGUF. На странице модели нужно выбрать "Use this model" и "Ollama", а затем получить команду для загрузки в консоли Ollama. Например:

```shell
ollama run hf.co/mistralai/Ministral-3-8B-Instruct-2512-GGUF:Q4_K_M
```

В описании модели есть список аппаратной совместимости. LLM любят VRAM и FP8: "_The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware. Ministral 3 8B can even be deployed locally, capable of fitting in 12GB of VRAM in FP8, and less if further quantized._"

Для задач edge deployment минимальная аппаратная рекомендация - использование дискретной видео-карты с 16 ГБ ОЗУ VRAM.
