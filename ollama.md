# Ollama - локальные LLM

[Ollama](https://ollama.com/) -  это инструмент для простого локального запуска больших языковых моделей (LLM) одной командой. Его часто сравнивают с Docker — но применительно к AI‑моделям: достаточно выполнить одну команду, и модель запускается без необходимости разбираться в зависимостях, конфигурациях и драйверах.

Пример команды:

```shell
ollama run llama2
```

Ollama работает с рядом известных открытых LLM, в том числе:

- LLaMA (включая Llama 3)
- Mistral
- CodeLlama

Ключевое свойство - автоматическая оптимизация:

- определяет доступные ресурсы (CPU/GPU)
- подстраивает загрузку модели под возможности оборудования
- поддерживает квантование и оптимизацию моделей

Небольшие модели можно запускать и на бюджетных процессорах, например, на Core i3 13100, но рекомендуется использовать только 3B‑модели (например, llama3.2:3b, phi3:mini):

```shell
# Запуск Phi‑3 Mini
ollama run phi3:mini

# Запуск Llama 3.2 3B
ollama run llama3.2:3b

# Запуск квантованной Mistral 7B
ollama run mistral:7b-instruct-v0.1-q4_k_m
```

Core i3‑13100 подходит для экспериментов с Ollama и LLM, но не более того.

Настройки для повышения производительности:

- количество параллельных запросов не более 1–2
- квантованные версии моделей (формат GGUF, 4–8 бит)
- все фоновые ресурсоёмкие приложения должны быть закрыты
- для снижения нагрузки следует уменьшить размер контекста. Например: `--num-ctx 2048`

## Hugging Face

[Hugging Face](https://huggingface.co/) - это тоже самое для машинного обучения, что GitHub для кода. На Hugging Face размещаются ИИ модели, решающие различные задачи. В частности, есть много узкоспециализированных ИИ моделей, например, только для генерации кода на ИИ. Узкоспециализированные модели могут эффективно решать сложные задачи, расходуя кратно меньшее количество ресурсов, чем при использовании универсальных моделей.

>Hugging Face после нескольких лет плодотворного сотрудничества купила команду llama.cpp. Предполагается, что ПО останется открытым, но у разработчиков появится больше ресурсов для совершенствования продуктов.

В репозитариях Hugging Face находятся как модели, так и Datasets.

>Hugging Face - это эмодзи "обнимашка". Миссия Hugging Face: "ИИ для исследователей, демократизация ИИ".

Для запуска моделей с Hugging Face необходима Ollama. Можно зарузить Docker-контейнер, но для этого нужне API-ключ (DASHSCOPE_API_KEY), а также поддержку containerd engine в Docker.

Благодаря использованию карточек моделей и поиска, на Hugging Face довольно удобно искать специализированные модели.

## Установка Ollama

Команда установки Ollama в Linux:

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

Проверка версии:

```shell
ollama --version
```
