# Тестовые задания Евгения (скопировал из его блокнота для проверки)

Подзадание №2: напишите итератор, который будет подавать в цикл переданное ему число элементов. Например, если мы итерируемся по списку [1, 2, 3, 4, 5], а на вход итератору передано число 2, он сначала вернёт (1, 2), затем (3, 4) и на этом завершит итерации.

Решение:

```py
class IntegerIterator:
    def __init__(self, int_list, int_count):
        if not isinstance(int_list, list) or not all(isinstance(i, int) for i in int_list):
            raise ValueError("Input must be a list of integers.")
        self.int_list = int_list
        self.index = 0

        # Сохраняем количество элементов, которые нужно возвращать на каждой итерации
        self.count = int_count

    def __iter__(self):
        return self

    def __next__(self):
        if self.index + self.count - 1 < len(self.int_list):
            result = self.int_list[self.index:self.index+self.count]
            self.index += self.count
            return tuple(result)
        else:
            raise StopIteration

# Пример использования
int_list = [1, 2, 3, 4, 5]
iterator = IntegerIterator(int_list, 2)

for number in iterator:
    print(number)
```

Подзадание №3: напишите класс, на вход которому при создании экземпляра подаётся список строк, в каждой из которых обязательно содержится как минимум 2 слова. Класс должен поддерживать следующие методы:

- get_tf(word, number of doc): принимает на вход слово и номер строки, возвращает значение TF
- get_idf(word, number of doc): принимает на вход те же самые параметры, возвращает значение IDF
- get_tf_idf(word, number of doc, ignore_stopwords=True): возвращает значение TF-IDF. По умолчанию при его вычислении не учитываются стопслова, если при вызове данного метода установить значение ignore_stopwords=False, учитывать стопслова при вычислении TF-IDF. Стопслова сделайте переменной класса.

Ценность тестового задания в двух вещах:

- усвоить разницу между классом и экземпляром класса. Акцент на использование **self**
- повторить основные парадигмы машинной обработки естественного языка (NLP), в том числе, ключевые метрики (TF, IDF, TF-IDF)

```py
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

class TextAnalyzer:
    def __init__(self, list_words):
        self.list_words = list_words
        self.stop_words = []

    # Вспомогательный метод - для уменьшения объёма класса
    def _get_estimation(self, word, number_of_doc, _use_idf):

        # Создание и настройка TF-векторизатора
        # use_idf=False отключает IDF, оставляя только TF
        #
        # Параметр norm=None означает, что не должна выполняться нормализация,
        # т.е. вычислять нужно абсолютные веса, а не а не относительные доли
        vectorizer = TfidfVectorizer(use_idf=_use_idf, norm=None)

        # Обучение векторизатора на данных: метод fit_transform анализирует 
        # список текстов (self.list_words), составляет словарь уникальных слов 
        # и строит матрицу, где строки соответствуют документам, а столбцы — словам.
        # Значения в ячейках этой матрицы — это вычисленные веса слов.
        tf_matrix = vectorizer.fit_transform(self.list_words)

        # Проверяем, есть ли указанное слово в подготовленной матрице.
        # Заметим, что feature_names - это массив numpy.ndarray, т.е.
        # выгруженный заголовок таблицы (матрицы)
        feature_names = vectorizer.get_feature_names_out()
        if word in feature_names:

            # Находим индекс столбца в матрице документов и слов
            word_index = list(feature_names).index(word)

            """
            # Код извлекает весь столбец значений для этого слова по всем документам. 
            # Символ двоеточия ":" это slice, т.е. эквивалент [:], в данном случае 
            # означающий берём все строки. При этом используется только один конкретный 
            # столбец. 
            # Mетод toarray() преобразует полученный столбец в массив NumPy. 
            # Метод flatten() преобразует массив NumPy в одномерный вектор - строку, 
            # выполяя "уплощение" данных:
            tf_values = tf_matrix[:, word_index].toarray().flatten()

            # Используя индекс документа мы получаем частоту (TF), или IDF указанного 
            # слова в документе
            return tf_values[number_of_doc]
            """

            # Более быстрый вариант кода, чем приведённый выше и закомментированный
            return tf_matrix[number_of_doc, word_index]
            
        return 0

    # Метод возвращает TF (Term Frequency) — Частота термина
    def get_tf(self, word, number_of_doc):
        return self._get_estimation(word, number_of_doc, False)

    # Метод возвращает IDF (Inverse Document Frequency) — Обратная частота документа
    def get_idf(self, word, number_of_doc):

        # binary=True означает, что слово либо есть (1), либо нет (0) в документе
        # Это даст нам IDF, умноженный на 1
        vectorizer = TfidfVectorizer(binary=True, norm=None, use_idf=True)
        tfidf_matrix = vectorizer.fit_transform(self.list_words)

        # Получить чистые IDF-значения
        feature_names = vectorizer.get_feature_names_out()

        if word in feature_names:
            idf_values = vectorizer.idf_
            return idf_values[number_of_doc]

        return 0

    # Метод возвращает TF-IDF = TF × IDF
    def get_tf_idf(self, word, number_of_doc, ignore_stopwords=True):
        return self._get_estimation(word, number_of_doc, True)

# Примеры использования
base_strings = ["apple banana plastic", "banana pineapple cherry banana", "cherry watermelon durian melon plastic", "date melon", "fig watermelon", "grape pear"]

obj = TextAnalyzer(base_strings)
obj.stop_words = ["plastic", "coal"]

val = obj.get_tf("banana", 1)
print(f"TF слова 'banana': {val}")

# Интерпретация IDF:
# Высокое значение IDF = слово редкое (встречается в малом количестве документов)
# Низкое значение IDF = слово частое (встречается во многих документах)
val = obj.get_idf("cherry", 2)
print(f"IDF слова 'cherry': {val}")

# Слово важно для документа, если оно часто встречается в этом документе (высокий TF),
# но при этом редко встречается в других документах (высокий IDF)
val = obj.get_tf_idf("durian", 2, False)
print(f"IDF слова 'durian': {val}")
```

Приведённый выше код оченб не эффективный. В реальном приложении нужно было бы подумать о том, чтобы проводить векторизацию только один раз - при создании экземпляра класса:

```py
class TextAnalyzer:
    def __init__(self, list_words, use_idf=True):
        self.list_words = list_words
        # 1. Инициализация и обучение происходят только один раз при создании объекта
        self.vectorizer = TfidfVectorizer(use_idf=use_idf, norm=None)
        self.tf_matrix = self.vectorizer.fit_transform(self.list_words)
```

Но в конкретном задании - это не вариант, т.к. требования к составу параметров методов класса требуют настройки векторизатора разным образом. Т.е. пересматривать нужно не только реализацию, но и семантику использования класса.

Также важно не перебирать список имен признаков, а использовать более эффективный доступ, например, хэш-таблицу. Это должно компенсировать низкую эффективность вот этого кода:

```py
feature_names = vectorizer.get_feature_names_out()
if word in feature_names:
```

### О чём эти тестовые задания

Анализ текста чаще всего решает следующие задачи:

- Поиск по документам — находим релевантные документы
- Классификация текстов — определяем тематику
- Извлечение ключевых слов — выделяем главные темы документа
- Рекомендательные системы — находим похожие тексты

**TF** (Term Frequency = Частота термина) показывает, как часто слово встречается в конкретном документе.

Пример - документ: "машинное обучение это интересно машинное обучение":

Слово "машинное" встречается 2 раза из 6 слов → TF = 2/6 = 0.33

Слово "это" встречается 1 раз из 6 слов → TF = 1/6 = 0.17

Смысл: чем чаще слово в документе, тем оно важнее для этого документа.

**IDF** (Inverse Document Frequency = Обратная частота документа) показывает, насколько редким является слово во всей коллекции документов.

Смысл: редкие слова получают больший вес, а частые слова (типа "и", "в", "это") — меньший.

Переменная _use_idf определяет, будет ли учитываться обратная частота документа (насколько редким является слово во всем корпусе).

**TF-IDF** = TF × IDF, объединяет обе метрики:

Логика:

- Слово важно для документа, если оно часто встречается в этом документе (высокий TF)
- НО при этом редко встречается в других документах (высокий IDF)

Пример использования - анализируем статью о машинном обучении:

- Слово "нейронная" — высокий TF-IDF (часто в статье + редко в других документах) → важное слово
- Слово "это" — низкий TF-IDF (даже если часто в статье, но встречается везде) → не важное слово

## Терминология

**Корпус** в контексте компьютерной лингвистики и обработки естественного языка (NLP) представляет собой большой, унифицированный и структурированный набор текстов, используемый для статистического анализа, проверки лингвистических гипотез или обучения моделей машинного обучения. Само слово происходит от латинского "corpus", что означает "тело", подразумевая "тело текстов" или свод материалов.

В задачах машинного обучения корпус выступает в роли обучающей выборки или базы знаний. Алгоритмы сканируют эти тексты, чтобы составить словарь, вычислить частоту употребления слов и выявить закономерности их совместного появления. Качество и разнообразие корпуса критически важны, так как модель не может "знать" слова или контексты, которые не встречались в исходном наборе данных.
